{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# converts string to a standardised array [firstname, lastname]\n",
    "def standardise(name):\n",
    "    name = name.replace(\"Dr.\",\"\").replace(\"Prof.\",\"\").replace(\" von\", \"\").replace(u'\\xa0'+\"von\",\"\").replace(\"von \", \"\").replace(\"von\"+u'\\xa0',\"\").replace(u'\\xa0',' ').lower().replace('ä','ae').replace('ö','oe').replace('ü','ue').replace('ß','ss')\n",
    "\n",
    "    firstname = \"\"\n",
    "    lastname = \"\"\n",
    "    if ',' in name:\n",
    "        name = name.split(',')\n",
    "        firstname = name[-1]\n",
    "        for string in firstname.split(' '):\n",
    "            if len(string) > 0:\n",
    "                firstname = string\n",
    "                break\n",
    "        lastname = name[0]\n",
    "    else:\n",
    "        name = name.split(' ')\n",
    "        for string in name:\n",
    "            if len(string) > 0:\n",
    "                if len(firstname) == 0:\n",
    "                    firstname = string\n",
    "                else:\n",
    "                    lastname = string\n",
    "        if len(lastname) == 0:\n",
    "            lastname = firstname\n",
    "            firstname = \"\"\n",
    "    if len(firstname) == 0:\n",
    "        firstname = \"?\"\n",
    "    return [firstname, lastname]\n",
    "\n",
    "# tries to find deputies in a string, returns a list with the indizes of deputies found\n",
    "def find_deputies(string, df):\n",
    "    string = string.lower().replace('ä','ae').replace('ö','oe').replace('ü','ue').replace('ß','ss')\n",
    "    df = df.apply(lambda row: row['firstname'] in string and row['lastname'] in string, axis = 1)\n",
    "    return df[df].index.tolist()\n",
    "\n",
    "# tries to find parties in a string, returns a list with the indizes of parties found\n",
    "def find_parties(string):\n",
    "    #remove parties in [...], these just describe a deputy:\n",
    "    string = re.sub(r'\\[[^\\]]+\\]', '', string)\n",
    "    possible_parties = ['CDU', 'SPD', 'GRÜNE', 'FDP', 'AFD', 'LINKE']\n",
    "    parties = []\n",
    "    #Search parties:\n",
    "    for i in range(len(possible_parties)):\n",
    "        if possible_parties[i] in string.upper():\n",
    "            parties.append(i)\n",
    "    return parties\n",
    "\n",
    "# tries to interpret a comment in plenar protocol, returns the the event(s) and links to deputies and parties\n",
    "def interpret_comment(comment, df_deputies, comment_id, speech_id):\n",
    "    # Arrays to save Series wich can be added to the data_frame\n",
    "    events = []\n",
    "    deputy_links = []\n",
    "    party_links = []\n",
    "\n",
    "    possible_types = ['Beifall', ': ', 'Lachen', 'Zuruf', 'Widerspruch', 'Heiterkeit']\n",
    "\n",
    "    comment = comment.replace(u'\\xa0',' ').replace('–','-')\n",
    "\n",
    "    # Multiple events are seperated by ' - ':\n",
    "    c = 0\n",
    "    for single_comment in comment.split(' - '):\n",
    "        event_id = str(comment_id)+\"_\"+str(c)\n",
    "\n",
    "        #Search parties:\n",
    "        parties = find_parties(single_comment)\n",
    "\n",
    "        #Search deputies:\n",
    "        deputies = find_deputies(single_comment, df_deputies)\n",
    "\n",
    "        # search event types:\n",
    "        event_type = 0\n",
    "        for possible_type in possible_types:\n",
    "            if possible_type in single_comment:\n",
    "                break;\n",
    "            event_type += 1\n",
    "\n",
    "        event = pd.Series([event_type, speech_id], name = event_id, index = ['event_type', 'speech_id'])\n",
    "        events.append(event)\n",
    "\n",
    "        for party in parties:\n",
    "            party_link = pd.Series([party,event_id], index = ['party_id', 'event_id'])\n",
    "            party_links.append(party_link)\n",
    "\n",
    "        for deputy in deputies:\n",
    "            deputy_link = pd.Series([deputy, event_id], index = ['deputy_id','event_id'])\n",
    "            deputy_links.append(deputy_link)\n",
    "\n",
    "        c += 1\n",
    "    return [events, deputy_links, party_links]\n",
    "\n",
    "\n",
    "# Loads a file, e.g. MDB_STAMMDATEN.XML. Returns a df of the deputies of Bundestag and a second df with their periods\n",
    "def load_deputies_from_xml(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    person_row_list = []\n",
    "\n",
    "    link_list = []\n",
    "    for child in root.findall('MDB'):\n",
    "        item = child.find('ID')\n",
    "        person_id = \"\" if item is None else int(item.text)\n",
    "        item = child.find('./NAMEN/NAME/VORNAME')\n",
    "        firstname = \"\" if item is None else item.text\n",
    "        item = child.find('./NAMEN/NAME/NACHNAME')\n",
    "        lastname = \"\" if item is None else item.text\n",
    "        item = child.find('./NAMEN/NAME/AKAD_TITEL')\n",
    "        titel = \"\" if item is None else item.text\n",
    "        item = child.find('./BIOGRAFISCHE_ANGABEN/GEBURTSORT')\n",
    "        geburtsort = \"\" if item is None else item.text\n",
    "        item = child.find('./BIOGRAFISCHE_ANGABEN/GEBURTSDATUM')\n",
    "        geburtsdatum = \"\" if item is None else item.text\n",
    "        item = child.find('./BIOGRAFISCHE_ANGABEN/STERBEDATUM')\n",
    "        sterbedatum = \"\" if item is None else item.text\n",
    "        item = child.find('./BIOGRAFISCHE_ANGABEN/BERUF')\n",
    "        berufe = \"\" if item is None or item.text is None else item.text.replace(' /',',').replace('/',',').replace(' und ',',').replace(', ',',').split(',')\n",
    "        item = child.find('./BIOGRAFISCHE_ANGABEN/PARTEI_KURZ')\n",
    "        fraktion = \"\" if item is None else item.text\n",
    "\n",
    "        for wahlperiode in child.findall('./WAHLPERIODEN/WAHLPERIODE'):\n",
    "            link_list.append(pd.Series([int(wahlperiode.find('WP').text),person_id], index = ['election_period', 'deputy_id']))\n",
    "\n",
    "        [firstname,lastname] = standardise(lastname+','+firstname)\n",
    "        person_row_list.append(pd.Series([firstname, lastname,titel,geburtsort,geburtsdatum,sterbedatum,fraktion,berufe], name = person_id, index = ['firstname','lastname', 'title', 'birthplace', 'birthdate', 'deathday', 'fraktion', 'jobs']))\n",
    "\n",
    "    df_deputies = pd.DataFrame(person_row_list)\n",
    "    dl2 = pd.DataFrame(link_list)\n",
    "\n",
    "    return [df_deputies, dl2]\n",
    "\n",
    "# loads a file, e.g. plenarprotokoll_xxxx.xml. Returns [session_row, dl_session_members, df_events], where all elements needs to be append\n",
    "# to the general dataframe\n",
    "def load_session_from_xml(path, df_deputies, dl_deputies_periods):\n",
    "    # Read Session meta-data and create a pd.Series containing that information:\n",
    "    session_id = int(path.split(\"_\")[-1].split('.')[0])\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    session_number = int(root.find('./vorspann/kopfdaten/plenarprotokoll-nummer/sitzungsnr').text)\n",
    "    election_period = int(root.find('./vorspann/kopfdaten/plenarprotokoll-nummer/wahlperiode').text)\n",
    "    date = root.find('./vorspann/kopfdaten/veranstaltungsdaten/datum').attrib['date']\n",
    "    session_row = pd.Series([election_period,session_number,date], name=session_id, index = ['election_period','session_number','date'])\n",
    "\n",
    "    # Create a list of participants\n",
    "    # All deputies of this period are there, only the 'entschuldigte Abgeordnete' are missing\n",
    "    df_participants = df_deputies.loc[dl_deputies_periods[dl_deputies_periods.election_period == election_period]['deputy_id']] # All deputies of this election period\n",
    "    entschuldigte_anlage = root.find('./anlagen/anlage/anlagen-text[@anlagen-typ=\"Entschuldigte Abgeordnete\"]')\n",
    "    missing_deputies = []\n",
    "    if entschuldigte_anlage is not None:\n",
    "        for tr in entschuldigte_anlage.findall('./table/tbody/tr'):\n",
    "            name = tr.find('td').text\n",
    "            missing_deputies += find_deputies(name,df_participants)\n",
    "    df_participants = df_participants.drop(missing_deputies) # drop deputies mentioned in \"Entschuldigte Abgeordnete\"\n",
    "    dl_session_members = pd.DataFrame(list(zip([session_id]*len(df_participants), df_participants.index.tolist()))) # Create data link\n",
    "    dl_session_members.columns = ['session_id', 'deputy_id']\n",
    "    # TODO: former deputies of this election period will still appear, maybe filter them by bundestag membership period?\n",
    "\n",
    "    # Create list of all events in this session\n",
    "    events = [] # Data Series of events\n",
    "    deputy_event_links = [] # Data Series of deputy_event - Links\n",
    "    party_event_links = [] # Data Series of party event links\n",
    "    speeches = [] # Data Series containing information about speeches\n",
    "\n",
    "    comment_id = 0 # counts comments in this session\n",
    "    for tagesordnungspunkt in root.findall('./sitzungsverlauf/tagesordnungspunkt'):\n",
    "        for kommentar in tagesordnungspunkt.findall('kommentar'):\n",
    "            [comment_events, comment_deputy_links, comment_party_links] = interpret_comment(kommentar.text, df_participants, str(session_id) + \"_\" + str(comment_id), 0) # use df_participants to reduce apearance of name duplications\n",
    "            events += comment_events\n",
    "            deputy_event_links += comment_deputy_links\n",
    "            party_event_links += comment_party_links\n",
    "            comment_id += 1\n",
    "        speech_list = tagesordnungspunkt.findall('rede')\n",
    "        for i in range(len(speech_list)):\n",
    "            # Speech id of actual speech:\n",
    "            speech_id = speech_list[i].attrib['id']\n",
    "            # Speaker in this speech:\n",
    "            speaker_id = speech_list[i].find('p[@klasse=\"redner\"]/redner').attrib['id']\n",
    "            speeches.append(pd.Series([speaker_id, session_id], name = speech_id, index = ['speaker_id','session_id']))\n",
    "            # Speech_id of next speech (if available):\n",
    "            next_speech_id = speech_list[i+1].attrib['id'] if i+1 < len(speech_list) else speech_id\n",
    "            comment_list = speech_list[i].findall('kommentar')\n",
    "            for j in range(len(comment_list)):\n",
    "                if j < len(comment_list) - 1:\n",
    "                    [comment_events, comment_deputy_links, comment_party_links] = interpret_comment(comment_list[j].text, df_participants, str(session_id) + \"_\" + str(comment_id), speech_id) # use df_participants to reduce apearance of name duplications\n",
    "                    events += comment_events\n",
    "                    deputy_event_links += comment_deputy_links\n",
    "                    party_event_links += comment_party_links\n",
    "                else: # Last comment in a speech is usually the applause for the next speeker\n",
    "                    [comment_events, comment_deputy_links, comment_party_links] = interpret_comment(comment_list[j].text, df_participants, str(session_id) + \"_\" + str(comment_id), next_speech_id) # use df_participants to reduce apearance of name duplications\n",
    "                    events += comment_events\n",
    "                    deputy_event_links += comment_deputy_links\n",
    "                    party_event_links += comment_party_links\n",
    "                comment_id += 1\n",
    "\n",
    "\n",
    "\n",
    "    return[session_row, dl_session_members, events, deputy_event_links, party_event_links, speeches]\n",
    "\n",
    "#loads all databases by fetching them from original files\n",
    "def fetch_all(mdb_path, plenar_paths):\n",
    "\n",
    "    df_deputies = None # DF with deputies: ['firstname','lastname', 'title', 'birthplace', 'birthdate', 'deathday', 'fraktion', 'jobs'], index = deputy_id\n",
    "    dl_deputy_periods = None # DL for deputies and election_period: ['election_period', 'deputy_id']\n",
    "    df_sessions = pd.DataFrame() # DF with parliament session information: ['election_period','session_number','date'], index = session_id\n",
    "    dl_session_deputy = pd.DataFrame() # DL for linking parliament sessions to deputies: ['session_id', 'deputy_id']\n",
    "    df_events = pd.DataFrame() # DF with events during sessions: ['event_type', 'speech_id'], index = ignore\n",
    "    df_speeches = pd.DataFrame() # DF with speeches: ['speaker_id', 'session_id']\n",
    "    dl_deputy_event = pd.DataFrame() # DL for linking deputies to events they participated in: ['deputy_id','event_id'], index = ignore\n",
    "    dl_party_event = pd.DataFrame() # DL for linking parties to events they participate in: ['party_id', 'event_id'], index = ignore\n",
    "\n",
    "    print(f'Fetching {mdb_path} ...')\n",
    "    [df_deputies, dl_deputy_periods] = load_deputies_from_xml(mdb_path)\n",
    "\n",
    "    for path in plenar_paths:\n",
    "        print(f'Fetching {path} ...')\n",
    "        [session_row, dl_session_members, events, deputy_event_links, party_event_links, speeches] = load_session_from_xml(path, df_deputies, dl_deputy_periods)\n",
    "        df_sessions = pd.concat([df_sessions.T,session_row], axis = 1).T\n",
    "        dl_session_deputy = pd.concat([dl_session_deputy.T, dl_session_members.T], axis = 1, ignore_index = True).T\n",
    "        df_events = pd.concat([df_events.T] + events, axis = 1).T\n",
    "        df_speeches = pd.concat([df_speeches.T] + speeches, axis = 1).T\n",
    "        dl_deputy_event = pd.concat([dl_deputy_event.T] + deputy_event_links, axis = 1, ignore_index = True).T\n",
    "        dl_party_event = pd.concat([dl_party_event.T] + party_event_links, axis = 1, ignore_index = True).T\n",
    "\n",
    "    return [df_deputies,df_sessions,df_events,df_speeches,dl_session_deputy,dl_deputy_event,dl_party_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  firstname lastname\n",
      "0       max    meier\n",
      "1      hans    wurst\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame([['max','meier'],['hans','wurst']], columns = ['firstname','lastname'])\n",
    "my_ds = pd.Series(['karl','mustermann'], name = 3, index = ['firstname','lastname'])#, pd.Series(['karl1','mustermann1'], name = 4, index = ['firstname','lastname'])]\n",
    "#print(pd.concat([my_df.T,my_ds], axis=1).T)\n",
    "print(my_df.loc[[0,1]])\n",
    "my_df.to_csv('../0_datasets/integrated_data/test.csv', index = True)\n",
    "#find_deputies(\"Max, Meier und der Wurst, Hans gingen auf die Toilette.\", my_df)\n",
    "\n",
    "#print(standardise('Merkel, Dr. Angela'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ../0_datasets/MDB_STAMMDATEN.XML ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5386.xml ...\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n",
      "....Event 1 in speech ID1920511800\n",
      "    deputy_id    event_id\n",
      "110  11004012  5386_234_0\n",
      "matthias birkwald\n",
      "....Event 1 in speech ID1920511800\n",
      "    deputy_id    event_id\n",
      "111  11004012  5386_235_0\n",
      "matthias birkwald\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n",
      "....Event 3 in speech ID1920511800\n",
      "    deputy_id    event_id\n",
      "112  11004401  5386_238_0\n",
      "dagmar schmidt\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n",
      "....Event 0 in speech ID1920511800\n",
      "    deputy_id    event_id\n",
      "113  11003888  5386_240_0\n",
      "wolfgang strengmann-kuhn\n",
      "....Event 1 in speech ID1920511800\n",
      "    deputy_id    event_id\n",
      "114  11004012  5386_241_0\n",
      "matthias birkwald\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n",
      "....Event 0 in speech ID1920511800\n",
      "Empty DataFrame\n",
      "Columns: [deputy_id, event_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "[df_deputies,df_sessions,df_events,df_speeches,dl_session_deputy,dl_deputy_event,dl_party_event] = fetch_all('../0_datasets/MDB_STAMMDATEN.XML', ['../0_datasets/bundestag/plenarprotokoll_5386.xml'])\n",
    "#print(df_events)\n",
    "#print(df_deputies)\n",
    "for index, row in df_events.iterrows():\n",
    "    if(row['speech_id'] != \"ID1920511800\"):\n",
    "        continue\n",
    "    print('....Event ' + str(row['event_type']) + \" in speech \" + str(row['speech_id']))\n",
    "    print(dl_deputy_event[dl_deputy_event['event_id'] == index])\n",
    "    for index2, deputy in df_deputies.filter( items = dl_deputy_event[dl_deputy_event['event_id'] == index]['deputy_id'], axis = 0).iterrows():\n",
    "        print(deputy['firstname'] + \" \" + deputy['lastname'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ../0_datasets/MDB_STAMMDATEN.XML ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5309.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5423.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5417.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5303.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5314.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5316.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5411.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5363.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5409.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5397.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5352.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5419.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5337.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5398.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5360.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5393.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5362.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5328.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5332.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5312.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5386.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5345.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5402.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5357.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5346.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5390.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5378.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5377.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5313.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5347.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5387.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5405.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5375.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5304.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5355.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5315.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5407.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5408.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5324.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5364.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5384.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5412.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5348.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5307.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5356.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5305.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5361.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5415.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5373.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5416.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5336.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5354.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5380.xml ...\n",
      "Fetching ../0_datasets/bundestag/plenarprotokoll_5353.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "mdb_path = '../0_datasets/MDB_STAMMDATEN.XML'\n",
    "protocol_paths = os.listdir('../0_datasets/bundestag')\n",
    "protocol_paths = ['../0_datasets/bundestag/{0}'.format(path) for path in protocol_paths]\n",
    "start = time.time()\n",
    "[df_deputies,df_sessions,df_events,df_speeches,dl_session_deputy,dl_deputy_event,dl_party_event] = fetch_all(mdb_path, protocol_paths)\n",
    "end = time.time()\n",
    "print(\"Total time: + \" + str(end-start))\n",
    "\n",
    "\n",
    "df_deputies.to_csv('../0_datasets/integrated_data/df_deputies.csv', index = True)\n",
    "df_sessions.to_csv('../0_datasets/integrated_data/df_sessions.csv', index = True)\n",
    "df_events.to_csv('../0_datasets/integrated_data/df_events.csv', index = True)\n",
    "df_speeches.to_csv('../0_datasets/integrated_data/df_speeches.csv', index = True)\n",
    "dl_session_deputy.to_csv('../0_datasets/integrated_data/dl_session_deputy.csv', index = False)\n",
    "dl_deputy_event.to_csv('../0_datasets/integrated_data/dl_deputy_event.csv', index = False)\n",
    "dl_party_event.to_csv('../0_datasets/integrated_data/dl_party_event.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
